# hnsw_builder.py
"""
ğŸ” HNSW Builder - Constructor del Ãndice Vectorial
Extrae funcionalidad de indexaciÃ³n de etl_pipeline.py

Responsabilidades:
- Cargar chunks procesados
- Construir Ã­ndice HNSW
- Guardar/cargar Ã­ndice
- BÃºsqueda vectorial eficiente

Autor: Rodrigo CÃ¡rdenas
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
import hnswlib
from sentence_transformers import SentenceTransformer


class HNSWBuilder:
    """Constructor del Ã­ndice HNSW para bÃºsqueda vectorial"""

    def __init__(
        self,
        embedding_model: str = "paraphrase-multilingual-mpnet-base-v2",
        dimension: int = 768,
        M: int = 16,
        ef_construction: int = 200,
        ef_search: int = 50,
    ):
        """
        Inicializa builder HNSW

        Args:
            embedding_model: Modelo de embeddings (multilingÃ¼e)
            dimension: DimensiÃ³n de embeddings (768 para multilingÃ¼e, 384 para simples)
            M: ParÃ¡metro M de HNSW (mayor = mÃ¡s conexiones, mÃ¡s memoria)
            ef_construction: ParÃ¡metro de construcciÃ³n (mayor = mejor calidad, mÃ¡s lento)
            ef_search: ParÃ¡metro de bÃºsqueda (mayor = mejor recall, mÃ¡s lento)
        """
        self.dimension = dimension
        self.M = M
        self.ef_construction = ef_construction
        self.ef_search = ef_search

        # Inicializar Ã­ndice HNSW
        self.index = hnswlib.Index(space="cosine", dim=dimension)
        self.index_initialized = False

        # Modelo de embeddings
        print(f"ğŸ”§ Cargando modelo de embeddings: {embedding_model}")
        self.model = SentenceTransformer(embedding_model)
        print(f"   âœ… Modelo cargado (dimensiÃ³n: {dimension})")

        # Mapeos
        self.id_to_chunk = {}
        self.chunk_counter = 0

    def initialize_index(self, max_elements: int = 10000):
        """
        Inicializa el Ã­ndice HNSW con capacidad mÃ¡xima

        Args:
            max_elements: NÃºmero mÃ¡ximo de elementos a indexar
        """
        print(f"\nğŸ”§ Inicializando Ã­ndice HNSW...")
        print(f"   Max elements: {max_elements}")
        print(f"   M: {self.M}")
        print(f"   ef_construction: {self.ef_construction}")

        self.index.init_index(
            max_elements=max_elements, ef_construction=self.ef_construction, M=self.M
        )

        self.index.set_ef(self.ef_search)
        self.index_initialized = True

        print(f"   âœ… Ãndice inicializado")

    def add_chunks_from_file(self, chunk_file: Path):
        """
        Agrega chunks desde un archivo JSON con embeddings

        Args:
            chunk_file: Ruta al archivo *_chunks.json
        """
        # Cargar chunks
        with open(chunk_file, "r", encoding="utf-8") as f:
            chunks = json.load(f)

        # Cargar embeddings
        embedding_file = chunk_file.parent / chunk_file.name.replace(
            "_chunks.json", "_embeddings.npy"
        )

        if not embedding_file.exists():
            print(f"   âš ï¸  Embeddings no encontrados: {embedding_file.name}")
            return 0

        embeddings = np.load(embedding_file)

        # Agregar al Ã­ndice
        added = 0
        for chunk, embedding in zip(chunks, embeddings):
            chunk_id = self.chunk_counter
            self.id_to_chunk[chunk_id] = chunk

            # Agregar al Ã­ndice HNSW
            self.index.add_items(embedding.reshape(1, -1), np.array([chunk_id]))

            self.chunk_counter += 1
            added += 1

        return added

    def build_from_directory(self, processed_dir: Path):
        """
        Construye Ã­ndice HNSW desde directorio de chunks procesados

        Args:
            processed_dir: Directorio con *_chunks.json y *_embeddings.npy
        """
        print(f"\nğŸ“‚ Construyendo Ã­ndice desde: {processed_dir}")

        # Buscar archivos de chunks
        chunk_files = list(processed_dir.glob("*_chunks.json"))

        if not chunk_files:
            print(f"âŒ No se encontraron chunks en {processed_dir}")
            return

        print(f"ğŸ“„ Encontrados {len(chunk_files)} archivos de chunks")

        # Contar total de chunks para inicializar Ã­ndice
        total_chunks = 0
        for chunk_file in chunk_files:
            try:
                with open(chunk_file, "r", encoding="utf-8") as f:
                    chunks = json.load(f)
                    total_chunks += len(chunks)
            except UnicodeDecodeError as e:
                print(f"   âš ï¸  Error de encoding en {chunk_file.name}: {e}")
                print(f"      Intentando con latin-1...")
                try:
                    with open(chunk_file, "r", encoding="latin-1") as f:
                        chunks = json.load(f)
                        total_chunks += len(chunks)
                except Exception as e2:
                    print(f"      âŒ No se pudo leer {chunk_file.name}: {e2}")
                    continue
            except Exception as e:
                print(f"   âš ï¸  Error leyendo {chunk_file.name}: {e}")
                continue

        # Inicializar Ã­ndice con capacidad suficiente
        self.initialize_index(max_elements=max(total_chunks, 10000))

        # Agregar chunks al Ã­ndice
        print(f"\nğŸ“¥ Agregando chunks al Ã­ndice...")

        for chunk_file in chunk_files:
            print(f"   ğŸ“– Procesando: {chunk_file.name}")
            added = self.add_chunks_from_file(chunk_file)
            print(f"      âœ… {added} chunks agregados")

        print(f"\nâœ… Ãndice HNSW construido: {self.chunk_counter} chunks totales")

    def save_index(self, filepath: Path):
        """
        Guarda el Ã­ndice HNSW en disco

        Args:
            filepath: Ruta donde guardar el Ã­ndice (ej: hnsw_index.bin)
        """
        if not self.index_initialized:
            print("âŒ Ãndice no inicializado, no se puede guardar")
            return

        # Guardar Ã­ndice HNSW
        filepath.parent.mkdir(parents=True, exist_ok=True)
        self.index.save_index(str(filepath))

        # Guardar mapeos
        mapping_file = filepath.parent / f"{filepath.stem}_mappings.json"
        with open(mapping_file, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "id_to_chunk": {str(k): v for k, v in self.id_to_chunk.items()},
                    "chunk_counter": self.chunk_counter,
                    "dimension": self.dimension,
                    "M": self.M,
                    "ef_construction": self.ef_construction,
                    "ef_search": self.ef_search,
                },
                f,
                indent=2,
                ensure_ascii=False,
            )

        print(f"ğŸ’¾ Ãndice HNSW guardado:")
        print(f"   Ãndice:   {filepath}")
        print(f"   Mappings: {mapping_file}")

    def load_index(self, filepath: Path):
        """
        Carga un Ã­ndice HNSW desde disco

        Args:
            filepath: Ruta al archivo del Ã­ndice
        """
        # Cargar Ã­ndice HNSW
        self.index.load_index(str(filepath))
        self.index_initialized = True

        # Cargar mapeos
        mapping_file = filepath.parent / f"{filepath.stem}_mappings.json"

        with open(mapping_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            self.id_to_chunk = {int(k): v for k, v in data["id_to_chunk"].items()}
            self.chunk_counter = data["chunk_counter"]

            # Restaurar parÃ¡metros
            self.dimension = data.get("dimension", self.dimension)
            self.M = data.get("M", self.M)
            self.ef_construction = data.get("ef_construction", self.ef_construction)
            self.ef_search = data.get("ef_search", self.ef_search)

        # Configurar ef de bÃºsqueda
        self.index.set_ef(self.ef_search)

        print(f"âœ… Ãndice HNSW cargado:")
        print(f"   Chunks: {self.chunk_counter}")
        print(f"   DimensiÃ³n: {self.dimension}")
        print(f"   ef_search: {self.ef_search}")

    def search(self, query: str, k: int = 10) -> List[Dict]:
        """
        Busca chunks similares a la query

        Args:
            query: Texto de consulta
            k: NÃºmero de resultados a retornar

        Returns:
            Lista de dicts con chunk_id, content, score, metadata
        """
        if not self.index_initialized:
            print("âŒ Ãndice no inicializado")
            return []

        # Generar embedding de query
        query_embedding = self.model.encode([query])[0]

        # Buscar en HNSW
        labels, distances = self.index.knn_query(query_embedding, k=k)

        # Construir resultados
        results = []
        for label, distance in zip(labels[0], distances[0]):
            chunk = self.id_to_chunk[label]
            results.append(
                {
                    "chunk_id": chunk["chunk_id"],
                    "content": chunk["content"],
                    "score": float(1 - distance),  # Convertir distancia a similitud
                    "doc_id": chunk["doc_id"],
                    "metadata": chunk.get("metadata", {}),
                }
            )

        return results


def main():
    """
    Script standalone para (re)construir Ã­ndice HNSW

    Uso tÃ­pico despuÃ©s de generar embeddings:
        python offline_etl/builders/hnsw_builder.py --input data/processed
    """
    import sys
    import argparse

    parser = argparse.ArgumentParser(
        description="Construye Ã­ndice HNSW desde chunks con embeddings"
    )

    parser.add_argument(
        "--input",
        type=Path,
        default=Path("data/processed"),
        help="Directorio con *_chunks.json y *_embeddings.npy",
    )

    parser.add_argument(
        "--output",
        type=Path,
        default=Path("data/indices/hnsw_index.bin"),
        help="Archivo de salida para Ã­ndice HNSW",
    )

    parser.add_argument(
        "--model",
        type=str,
        default="paraphrase-multilingual-mpnet-base-v2",
        help="Modelo de embeddings (debe coincidir con el usado para generar .npy)",
    )

    args = parser.parse_args()

    print("=" * 70)
    print("ğŸ” CONSTRUCCIÃ“N DE ÃNDICE HNSW")
    print("=" * 70)
    print(f"\nğŸ“‚ Input: {args.input}")
    print(f"ğŸ’¾ Output: {args.output}")
    print(f"ğŸ¤– Modelo: {args.model}\n")

    # Crear builder
    builder = HNSWBuilder(embedding_model=args.model)

    # Construir Ã­ndice
    print("ğŸ”§ Construyendo Ã­ndice desde embeddings...")
    builder.build_from_directory(args.input)

    if builder.chunk_counter == 0:
        print("\nâŒ No se encontraron embeddings")
        print("   Primero genera embeddings con:")
        print("   python utils/generate_embeddings.py --input data/processed --batch-size 4")
        sys.exit(1)

    # Guardar Ã­ndice
    print(f"\nğŸ’¾ Guardando Ã­ndice...")
    builder.save_index(args.output)

    print("\n" + "=" * 70)
    print("âœ… ÃNDICE HNSW CONSTRUIDO EXITOSAMENTE")
    print("=" * 70)
    print(f"\nğŸ“Š EstadÃ­sticas:")
    print(f"   Chunks indexados: {builder.chunk_counter}")
    print(f"   DimensiÃ³n: {builder.dimension}")
    print(f"   Archivo: {args.output}")
    print(f"\nğŸ‰ Â¡Listo para bÃºsquedas hÃ­bridas!")


if __name__ == "__main__":
    main()
